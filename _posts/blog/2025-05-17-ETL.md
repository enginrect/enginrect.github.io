---
title: ETL(Workflow)
description: 데이터를 추출·변환·적재하는 자동화 워크플로우
author: jay
date: 2025-05-17 12:00:00 +0800
categories: [Blogging]
tags: [blog, devops]
pin: false
math: true
mermaid: true
---

### 개념 설명:
> 추출, 전환, 적재(ETL)는 다양한 소스의 데이터를 데이터 웨어하우스라고 부르는 대형 중앙 집중식 리포지토리에 결합하는 과정입니다. ETL은 원시 데이터를 정리 및 구성해서 스토리지, 데이터 분석, 기계 학습(ML)용으로 준비하기 위한 비즈니스 규칙 세트입니다. 사용자는 데이터 분석(비즈니스 의사 결정의 결과 예측, 보고서 및 대시보드 생성, 운영 비효율성 저감 등)을 통해 특정 비즈니스 인텔리전스 요구 사항을 해결할 수 있습니다. \
> [AWS - 추출 변환 적재(ETL)란 무엇인가요?](https://aws.amazon.com/ko/what-is/etl/)

- **Extract**: 다양한 소스(RDB, API, 파일 등)에서 데이터 수집
- **Transform**: 정제, 필터링, 집계 등의 변환 로직 적용
- **Load**: 대상 시스템(DB, DW, S3 등)으로 적재


#### **vs ELT**: 
ELT는 데이터를 먼저 적재한 후 변환을 수행하는 방식으로, BigQuery, Snowflake 등 클라우드 기반 DW에서 자주 사용됨
#### **vs 데이터 스트리밍**: 
ETL은 주로 배치 중심, 스트리밍은 실시간 처리 중심

### WHEN?
- **배치 처리 기반의 데이터 파이프라인이 필요한 경우**
- **정기적으로 반복되는 데이터 수집·가공 작업이 있는 경우**
- **데이터 웨어하우스 구축이나 BI 도구 연동 목적이 있을 때**

### HOW? (example)

#### 1. 워크플로우 설계 도구 선택
- **Apache Airflow**: Python 기반 DAG(Directed Acyclic Graph) 정의
- **AWS Glue**: Serverless 방식으로 스크립트 기반 ETL 지원
- **Apache NiFi**: GUI 기반 데이터 흐름 정의

#### 2. 예시 (Airflow로 MySQL → S3 적재)

```python
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime

def extract():
    print("MySQL에서 데이터 추출")

def transform():
    print("데이터 변환")

def load():
    print("S3 버킷에 적재 - s3://enginrect-data/etl/")

with DAG('enginrect_etl_pipeline', start_date=datetime(2023, 1, 1), schedule_interval='@daily') as dag:
    t1 = PythonOperator(task_id='extract', python_callable=extract)
    t2 = PythonOperator(task_id='transform', python_callable=transform)
    t3 = PythonOperator(task_id='load', python_callable=load)

    t1 >> t2 >> t3
```

#### 3. Tip! (운영 시 고려사항)
- **재시도 및 알림 설정**: 실패 시 Slack, Email 알림
- **로그 관리 및 모니터링**: 로그 기반 에러 파악 및 지표 수집
- **스케줄 관리**: cron 표현식 또는 @hourly, @daily 등


